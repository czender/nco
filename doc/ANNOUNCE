$Header$ -*-text-*-

The netCDF Operators NCO version 5.0.1 have arrived.

http://nco.sf.net (Homepage, Mailing lists, Help)
http://github.com/nco (Source Code, Issues, Releases, Developers)

What's new?
Version 5.0.1 contains the necessary options for ncremap to exploit
MPI-enabled weight generators such as ESMF_RegridWeightGen.
You can probably skip this release unless you use ncremap.

Work on NCO 5.0.2 has commenced and and it targeted to offer ncremap
support for the MOAB offline regridding package mbtempest.

Enjoy,
Charlie

NEW FEATURES (full details always in ChangeLog):

A. ncremap accepts two new and related options, mpi_pfx and mpi_nbr.
  The --mpi_pfx=mpi_pfx option specifies an appropriate job
  scheduler prefix for MPI-enabled weight-generation
  executables such as ESMF's ESMF_RegridWeightGen and MoabTempest's
  mbtempest. Other weight generators (ncks, GenerateOfflineMap) 
  are unaffected by this option since they are not MPI-enabled. 
  mpi_pfx defaults to mpirun -n $mpi_nbr on all machines except those
  whose $HOSTNAME matches an internal database of DOE-operated
  supercomputers where mpi_pfx defaults to srun -n $mpi_nbr.
  When invoking --mpi_pfx, be sure to explicitly define the
  number of MPI tasks-per-node, e.g.,
  
  ncremap --mpi_pfx='srun -n 16' ...
  ncremap --mpi_pfx='srun --mpi=pmi2 -n 4' ...
  
  The separate --mpi_nbr=mpi_nbr option specifies the
  number of tasks-per-node that MPI-enabled weight generators
  will request. It preserves the default job scheduler prefix (srun or 
  mpirun):
  
  ncremap --mpi_nbr=4 ... # 16 MPI tasks-per-node for ERWG/mbtempest 
  ncremap --mpi_nbr=16 ... # 4 MPI tasks-per-node for ERWG/mbtempest 
  
  Thus --mpi_nbr=mpi_nbr can be used to create host-independent ncremap
  commands to facilitate benchmarking the scaling of weight-generators
  across hosts that work with the default value of mpi_pfx.
  The --mpi_pfx option will prevail and --mpi_nbr will be ignored if
  both are used in the same ncremap invocation. Note that mpi_pfx is
  only used internally by ncremap to exploit the MPI capabilities of
  select weight-generators. It is not used to control and does not
  affect the distribution of multiple ncremap commands among a cluster
  of nodes. 
  http://nco.sf.net/nco.html#mpi_pfx
  http://nco.sf.net/nco.html#mpi_nbr
  
BUG FIXES:
   
A. No bug fixes this release
    
Full release statement at http://nco.sf.net/ANNOUNCE
    
KNOWN PROBLEMS DUE TO NCO:

This section of ANNOUNCE reports and reminds users of the
existence and severity of known, not yet fixed, problems. 
These problems occur with NCO 5.0.1 built/tested under
MacOS 11.5 with netCDF 4.8.0 on HDF5 1.12.0 and with
Linux with netCDF 4.8.0 on HDF5 1.8.19.

A. NOT YET FIXED (NCO problem)
   Correctly read arrays of NC_STRING with embedded delimiters in ncatted arguments

   Demonstration:
   ncatted -D 5 -O -a new_string_att,att_var,c,sng,"list","of","str,ings" ~/nco/data/in_4.nc ~/foo.nc
   ncks -m -C -v att_var ~/foo.nc

   20130724: Verified problem still exists
   TODO nco1102
   Cause: NCO parsing of ncatted arguments is not sophisticated
   enough to handle arrays of NC_STRINGS with embedded delimiters.

B. NOT YET FIXED (NCO problem?)
   ncra/ncrcat (not ncks) hyperslabbing can fail on variables with multiple record dimensions

   Demonstration:
   ncrcat -O -d time,0 ~/nco/data/mrd.nc ~/foo.nc

   20140826: Verified problem still exists
   20140619: Problem reported by rmla
   Cause: Unsure. Maybe ncra.c loop structure not amenable to MRD?
   Workaround: Convert to fixed dimensions then hyperslab

KNOWN PROBLEMS DUE TO BASE LIBRARIES/PROTOCOLS:

A. NOT YET FIXED (netCDF4 or HDF5 problem?)
   Specifying strided hyperslab on large netCDF4 datasets leads
   to slowdown or failure with recent netCDF versions.

   Demonstration with NCO <= 4.4.5:
   time ncks -O -d time,0,,12 ~/ET_2000-01_2001-12.nc ~/foo.nc
   Demonstration with NCL:
   time ncl < ~/nco/data/ncl.ncl   
   20140718: Problem reported by Parker Norton
   20140826: Verified problem still exists
   20140930: Finish NCO workaround for problem
   20190201: Possibly this problem was fixed in netCDF 4.6.2 by https://github.com/Unidata/netcdf-c/pull/1001
   Cause: Slow algorithm in nc_var_gets()?
   Workaround #1: Use NCO 4.4.6 or later (avoids nc_var_gets())
   Workaround #2: Convert file to netCDF3 first, then use stride
   Workaround #3: Compile NCO with netCDF >= 4.6.2

B. NOT YET FIXED (netCDF4 library bug)
   Simultaneously renaming multiple dimensions in netCDF4 file can corrupt output

   Demonstration:
   ncrename -O -d lev,z -d lat,y -d lon,x ~/nco/data/in_grp.nc ~/foo.nc # Completes but produces unreadable file foo.nc
   ncks -v one ~/foo.nc

   20150922: Confirmed problem reported by Isabelle Dast, reported to Unidata
   20150924: Unidata confirmed problem
   20160212: Verified problem still exists in netCDF library
   20160512: Ditto
   20161028: Verified problem still exists with netCDF 4.4.1
   20170323: Verified problem still exists with netCDF 4.4.2-development
   20170323: https://github.com/Unidata/netcdf-c/issues/381
   20171102: Verified problem still exists with netCDF 4.5.1-development
   20171107: https://github.com/Unidata/netcdf-c/issues/597
   20190202: Progress has recently been made in netCDF 4.6.3-development
   More details: http://nco.sf.net/nco.html#ncrename_crd

C. NOT YET FIXED (would require DAP protocol change?)
   Unable to retrieve contents of variables including period '.' in name
   Periods are legal characters in netCDF variable names.
   Metadata are returned successfully, data are not.
   DAP non-transparency: Works locally, fails through DAP server.

   Demonstration:
   ncks -O -C -D 3 -v var_nm.dot -p http://thredds-test.ucar.edu/thredds/dodsC/testdods in.nc # Fails to find variable

   20130724: Verified problem still exists. 
   Stopped testing because inclusion of var_nm.dot broke all test scripts.
   NB: Hard to fix since DAP interprets '.' as structure delimiter in HTTP query string.

   Bug tracking: https://www.unidata.ucar.edu/jira/browse/NCF-47

D. NOT YET FIXED (would require DAP protocol change)
   Correctly read scalar characters over DAP.
   DAP non-transparency: Works locally, fails through DAP server.
   Problem, IMHO, is with DAP definition/protocol

   Demonstration:
   ncks -O -D 1 -H -C -m --md5_dgs -v md5_a -p http://thredds-test.ucar.edu/thredds/dodsC/testdods in.nc

   20120801: Verified problem still exists
   Bug report not filed
   Cause: DAP translates scalar characters into 64-element (this
   dimension is user-configurable, but still...), NUL-terminated
   strings so MD5 agreement fails 

"Sticky" reminders:

A. Reminder that NCO works on most HDF4 and HDF5 datasets, e.g., 
   HDF4: AMSR MERRA MODIS ...
   HDF5: GLAS ICESat Mabel SBUV ...
   HDF-EOS5: AURA HIRDLS OMI ...

B. Pre-built executables for many OS's at:
   http://nco.sf.net#bnr

